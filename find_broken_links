# c:\Users\steph\Python\first_project\get_sitemap_links.py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import deque
import logging  # Import the logging module

# Set up logging
logging.basicConfig(level=logging.INFO)

# The base URL of the website to check
BASE_URL = "https://www.example.com"

def find_broken_links(base_url):
    """
    Check if a sitemap exists for the website and extract all links from it.
    """
    sitemap_url = urljoin(base_url, "/sitemap.xml")
    try:
        response = requests.get(sitemap_url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "xml")
            return [loc.text for loc in soup.find_all("loc")]
        else:
            logging.warning(f"No sitemap found or inaccessible: {sitemap_url}, status: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"Error accessing sitemap: {e}")
    return []

def get_links_from_page(url):
    """
    Extract all valid links from a given webpage.
    """
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            links = set()
            for a_tag in soup.find_all("a", href=True):
                href = a_tag['href']
                full_url = urljoin(url, href)  # Resolve relative URLs
                if is_valid_url(full_url):
                    links.add(full_url)
            return links
        else:
            logging.warning(f"Failed to fetch page: {url}, status: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"Error fetching page {url}: {e}")
    return set()

def is_valid_url(url):
    """
    Check if a URL is valid and within the same domain as the base URL.
    """
    parsed_url = urlparse(url)
    base_domain = urlparse(BASE_URL).netloc
    return parsed_url.scheme in {"http", "https"} and parsed_url.netloc == base_domain

def check_link_status(url):
    """
    Check the HTTP status of a given link.
    """
    try:
        response = requests.head(url, allow_redirects=True, timeout=10)
        return response.status_code
    except requests.RequestException as e:
        logging.error(f"Error checking link {url}: {e}")
        return None

def crawl_website(base_url):
    """
    Crawl the website, checking all links for broken ones.
    """
    visited = set()  # Keep track of visited URLs
    to_visit = deque()  # Queue for URLs to visit

    # Check for a sitemap first
    sitemap_links = find_broken_links(base_url)
    if sitemap_links:
        logging.info(f"Using sitemap with {len(sitemap_links)} links.")
        to_visit.extend(sitemap_links)
    else:
        logging.info("No sitemap found, starting from the homepage.")
        to_visit.append(base_url)

    broken_links = []  # Keep track of broken links

    while to_visit:
        current_url = to_visit.popleft()  # Get the next URL to visit
        if current_url in visited:
            continue

        logging.info(f"Visiting: {current_url}")
        visited.add(current_url)

        # Get all links from the current page
        links = get_links_from_page(current_url)
        for link in links:
            if link not in visited:
                to_visit.append(link)

        # Check the link's status
        status = check_link_status(current_url)
        if status is None or status >= 400:
            logging.error(f"Broken link found: {current_url} (status: {status})")
            broken_links.append(current_url)

    # Write broken links to a file
    with open("broken_links.txt", "w") as file:
        for link in broken_links:
            file.write(f"{link}\n")

if __name__ == "__main__":
    # Call the crawl_website function with the base URL
    crawl_website(BASE_URL)